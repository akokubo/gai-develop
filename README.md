# ローカルLLMアプリ開発環境の構築

ローカルLLMの実行にはOllamaを使用。
LM Studioで構築することも可能だが、その方法はこのドキュメントでは触れない。

Dockerは使用していない。
Dockerを使用することも可能だが、その方法はこのドキュメントでは触れない。

* [Windows WSL(Ubuntu)でのvenv環境における開発](wsl/index.md)
* [macOSターミナルでのminiconda環境における開発](macos/index.md)

## 作者
[小久保 温(こくぼ・あつし)](https://akokubo.github.io/)

## ライセンス
[MIT License](LICENSE)
